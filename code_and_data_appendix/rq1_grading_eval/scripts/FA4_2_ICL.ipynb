{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hMounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openai==0.27.0 --quiet\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "import openai\n",
    "openai.api_key = userdata.get(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "model_ids = [model.id for model in openai.Model.list().data]\n",
    "assert MODEL in model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change FA and ABLATION in between ablations for same FA\n",
    "FA = \"4\"\n",
    "ABLATION = f\"_2_ICL\"\n",
    "\n",
    "DATA_PATH = \"\"\n",
    "PROMPT_PATH = \"\"\n",
    "RESULTS_PATH = \"\"\n",
    "\n",
    "ROLE_CONTENT_DELIM = \"!~*~!\"\n",
    "\n",
    "# Add this if going from I/O to ICL\n",
    "LINE_DELIM = \"\\n!@#@!\\n\"\n",
    "\n",
    "SEED = 312\n",
    "N_BOOTSTRAP = 5000\n",
    "\n",
    "# Number of few-shot instances * 2 + 1\n",
    "N_PROMPT_MESSAGES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'system!~*~!You are a helpful teacher\\'s assistant whose job it is to score middle school students\\' short answer formative assessment question responses in the Earth Science domain.\\n\\nIn this formative assessment, students complete an engineering task that requires them to evaluate two designs of a schoolyard on a 4x4 grid pursuant to several constraints.\\n\\nStudents are presented with the following information:\\n\\nA fictitious student, Morgan, has created two different designs. Morgan wants to test both of her designs to see which one is better.\\n\\nMorgan tested her first design (Design1) with the following inputs:\\nRainfall_Inches = 6\\nBuilding_Squares = 6\\nGrassy_Squares = 4\\nPlay_Squares = 3\\nParking_Squares = 3\\nAccessible_Squares = 8\\n\\nDesign1 produced the following outputs:\\nRunoff_Inches = 5.49\\nCost = $732,000\\n\\nMorgan tested her second design (Design2) with the following inputs:\\nRainfall_Inches = 3\\nBuilding_Squares = 4\\nGrassy_Squares = 3\\nPlay_Squares = 4\\nParking_Squares = 6\\nAccessible_Squares = 5\\n\\nDesign2 produced the following outputs:\\nRunoff_Inches = 2.72\\nCost = $695,000\\n\\nThe Engineering Constraints are as follows:\\nRunoff_Constraint: The design must minimize the amount of runoff.\\nAccessibility_Constraint: The design must include at least 6 accessible squares.\\nCost_Constraint: The cost of the design must be $750,000 or less.\\n\\nBased on this information, students are asked the following question:\\n\\nCan Morgan tell which design is better based on these tests? Explain why or why not.\\n\\nStudents\\' responses adhere to the following format:\\nAnswer: [Yes or No]\\nExplanation: [Justification for Answer]\\n\\nThe correct Answer is \\'No\\'. Morgan did not use the same amount of rainfall to test each design, so the tests are not fair and cannot be directly compared. Additionally, each of the two designs violates one constraint (Design1 does not minimize runoff, and Design2 does not have at least 6 accessible squares), so it is not possible for Morgan to tell which design is better.\\n\\nYou are to score student responses on a scale of 0 to 4 points based on the following Rubric:\\n4 points: The student indicates the two designs cannot be compared because Morgan used different rainfall values to test each design AND the student explains that the tests are not fair.\\n3 points: The student indicates the two designs cannot be compared because the rainfall values are different but does not mention that the tests are not fair.\\n2 points: The student indicates the two designs cannot be compared because each design violates one of the Engineering Constraints but does not mention the two tests have different amounts of rainfall.\\n1 point: The student indicates the two designs cannot be compared but does not provide reasoning for his or her Answer.\\n0 points: The student says the two designs can be compared, or the student provides any other response not meeting the Rubric criteria for 1, 2, 3, or 4 points.\\n\\nBased on all of this information, score the following student\\'s formative assessment response pursuant to the Rubric. Please provide your scores as a JSON document that conforms to the following schema:\\n\\n{\\n\\t\"score\" : int\\n}\\n!@#@!\\nuser!~*~!No\\n\\nMorgan cannot tell which of the two designs is better based on these tests because the designs use different input (i.e., rainfall) values. This means that the tests are not fair because runoff cannot be equally compared for the two tests if different amounts of rainfall are used.\\n!@#@!\\nassistant!~*~!{\\n\\t\"score\" : 4\\n}\\n!@#@!\\nuser!~*~!Yes\\n\\nDesign1 is better\\n!@#@!\\nassistant!~*~!{\\n\\t\"score\" : 0\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(PROMPT_PATH, 'r', encoding='utf-8') as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(messages):\n",
    "  start_time = time.time()\n",
    "  response = openai.ChatCompletion.create(\n",
    "      model=MODEL,\n",
    "      messages=messages,\n",
    "      temperature=0,\n",
    "      response_format={\"type\": \"json_object\"},\n",
    "      seed=SEED)\n",
    "  total_time = time.time()-start_time\n",
    "  total_tokens = response[\"usage\"][\"total_tokens\"]\n",
    "  generation = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "  return generation, total_time, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\n    \"working\": true,\\n    \"explanation\": \"The API is functioning correctly as there are no reported issues or errors in the current environment.\"\\n}', 0.6875457763671875, 64)\n"
     ]
    }
   ],
   "source": [
    "response = get_openai_response([{\"role\":\"system\",\"content\":\"Confirm that the api is working. Respond using the following JSON schema: {'working':bool, 'explanation':str}\"}])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED FA4 STUDENT 0.\n",
      "FINISHED FA4 STUDENT 1.\n",
      "FINISHED FA4 STUDENT 2.\n",
      "FINISHED FA4 STUDENT 3.\n",
      "FINISHED FA4 STUDENT 4.\n",
      "FINISHED FA4 STUDENT 5.\n",
      "FINISHED FA4 STUDENT 6.\n",
      "FINISHED FA4 STUDENT 7.\n",
      "FINISHED FA4 STUDENT 8.\n",
      "FINISHED FA4 STUDENT 9.\n",
      "FINISHED FA4 STUDENT 10.\n",
      "FINISHED FA4 STUDENT 11.\n",
      "FINISHED FA4 STUDENT 12.\n",
      "FINISHED FA4 STUDENT 13.\n",
      "FINISHED FA4 STUDENT 14.\n",
      "FINISHED FA4 STUDENT 15.\n",
      "FINISHED FA4 STUDENT 16.\n",
      "FINISHED FA4 STUDENT 17.\n",
      "FINISHED FA4 STUDENT 18.\n",
      "FINISHED FA4 STUDENT 19.\n",
      "FINISHED FA4 STUDENT 20.\n",
      "FINISHED FA4 STUDENT 21.\n",
      "FINISHED FA4 STUDENT 22.\n",
      "FINISHED FA4 STUDENT 23.\n",
      "FINISHED FA4 STUDENT 24.\n",
      "FINISHED FA4 STUDENT 25.\n",
      "FINISHED FA4 STUDENT 26.\n",
      "FINISHED FA4 STUDENT 27.\n",
      "FINISHED FA4 STUDENT 28.\n",
      "FINISHED FA4 STUDENT 29.\n",
      "FINISHED FA4 STUDENT 30.\n",
      "FINISHED FA4 STUDENT 31.\n",
      "FINISHED FA4 STUDENT 32.\n",
      "FINISHED FA4 STUDENT 33.\n",
      "FINISHED FA4 STUDENT 34.\n",
      "FINISHED FA4 STUDENT 35.\n",
      "FINISHED FA4 STUDENT 36.\n",
      "FINISHED FA4 STUDENT 37.\n",
      "FINISHED FA4 STUDENT 38.\n",
      "FINISHED FA4 STUDENT 39.\n",
      "FINISHED FA4 STUDENT 40.\n",
      "FINISHED FA4 STUDENT 41.\n",
      "FINISHED FA4 STUDENT 42.\n",
      "FINISHED FA4 STUDENT 43.\n",
      "FINISHED FA4 STUDENT 44.\n",
      "FINISHED FA4 STUDENT 45.\n",
      "FINISHED FA4 STUDENT 46.\n",
      "FINISHED FA4 STUDENT 47.\n",
      "FINISHED FA4 STUDENT 48.\n",
      "FINISHED FA4 STUDENT 49.\n"
     ]
    }
   ],
   "source": [
    "results = [[\"llm_score\",\"total_time_s\",\"total_tokens\"]]\n",
    "\n",
    "prompt_messages = prompt.split(LINE_DELIM)\n",
    "assert len(prompt_messages) == N_PROMPT_MESSAGES\n",
    "\n",
    "system_role, system_content = prompt_messages[0].split(ROLE_CONTENT_DELIM)\n",
    "user_role1, user_content1 = prompt_messages[1].split(ROLE_CONTENT_DELIM)\n",
    "assistant_role1, assistant_content1 = prompt_messages[2].split(ROLE_CONTENT_DELIM)\n",
    "user_role2, user_content2 = prompt_messages[3].split(ROLE_CONTENT_DELIM)\n",
    "assistant_role2, assistant_content2 = prompt_messages[4].split(ROLE_CONTENT_DELIM)\n",
    "\n",
    "for idx,row in df.iterrows():\n",
    "  messages = [\n",
    "      {\"role\":system_role,\"content\":system_content},\n",
    "      {\"role\":user_role1,\"content\":user_content1},\n",
    "      {\"role\":assistant_role1,\"content\":assistant_content1},\n",
    "      {\"role\":user_role2,\"content\":user_content2},\n",
    "      {\"role\":assistant_role2,\"content\":assistant_content2},\n",
    "      {\"role\":\"user\",\"content\":row['response']}\n",
    "  ]\n",
    "\n",
    "  generation, total_time, total_tokens = get_openai_response(messages)\n",
    "  generation_data = json.loads(generation)\n",
    "\n",
    "  results.append([generation_data[\"score\"],total_time,total_tokens])\n",
    "\n",
    "  print(f\"FINISHED FA{FA} STUDENT {idx}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results[1:],columns=results[0])\n",
    "df = pd.concat([df,df_results],axis=1)\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf=RESULTS_PATH,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro‑F1: 0.7800\n",
      "QWK:      0.8069\n"
     ]
    }
   ],
   "source": [
    "y_true = df[\"score\"].astype(int)\n",
    "y_pred = df[\"llm_score\"].astype(int)\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "\n",
    "print(f\"Micro‑F1: {f1:.4f}\")\n",
    "print(f\"QWK:      {qwk:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro‑F1: 78.00 ± 11.02\n",
      "QWK:      80.69 ± 15.69\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "f1_samples   = []\n",
    "kappa_samples = []\n",
    "\n",
    "n = len(df)\n",
    "\n",
    "for _ in range(N_BOOTSTRAP):\n",
    "    idx = rng.integers(0, n, n)\n",
    "    y_t = y_true.iloc[idx].values\n",
    "    y_p = y_pred.iloc[idx].values\n",
    "\n",
    "    f1_samples.append(\n",
    "        f1_score(y_t, y_p, average=\"micro\")\n",
    "    )\n",
    "    kappa_samples.append(\n",
    "        cohen_kappa_score(y_t, y_p, weights=\"quadratic\")\n",
    "    )\n",
    "\n",
    "ci_f1   = np.percentile(f1_samples, [2.5, 97.5])\n",
    "ci_kappa = np.percentile(kappa_samples, [2.5, 97.5])\n",
    "\n",
    "moe_f1   = (ci_f1[1]   - ci_f1[0])   / 2\n",
    "moe_kappa = (ci_kappa[1] - ci_kappa[0]) / 2\n",
    "\n",
    "print(f\"Micro‑F1: {f1*100:.2f} ± {moe_f1*100:.2f}\")\n",
    "print(f\"QWK:      {qwk*100:.2f} ± {moe_kappa*100:.2f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
