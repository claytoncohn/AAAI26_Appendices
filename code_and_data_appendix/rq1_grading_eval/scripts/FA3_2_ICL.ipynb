{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m61.4/70.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hMounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openai==0.27.0 --quiet\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "import openai\n",
    "openai.api_key = userdata.get(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "model_ids = [model.id for model in openai.Model.list().data]\n",
    "assert MODEL in model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change FA and ABLATION in between ablations for same FA\n",
    "FA = \"3\"\n",
    "ABLATION = f\"_2_ICL\"\n",
    "\n",
    "FA_COLS = [\n",
    "    \"incorrectly_set_absorption_limit\",\n",
    "    \"rainfall_compared_to_absorption\",\n",
    "    \"absorption_set_to_absorption_limit\",\n",
    "    \"greater_than_nested_in_less_than\",\n",
    "    \"absorption_limit_swapped_with_absorption\"\n",
    "]\n",
    "\n",
    "FA_COLS_DICT = {k: None for k in FA_COLS}\n",
    "\n",
    "DATA_PATH = \"\"\n",
    "PROMPT_PATH = \"\"\n",
    "RESULTS_PATH = \"\"\n",
    "\n",
    "ROLE_CONTENT_DELIM = \"!~*~!\"\n",
    "\n",
    "# Add this if going from I/O to ICL\n",
    "LINE_DELIM = \"\\n!@#@!\\n\"\n",
    "\n",
    "SEED = 312\n",
    "N_BOOTSTRAP = 5000\n",
    "\n",
    "# Number of few-shot instances * 2 + 1\n",
    "N_PROMPT_MESSAGES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'system!~*~!You are a helpful teacher\\'s assistant whose job it is to score middle school student short answer formative assessment question responses in the Earth Science domain.\\n\\nIn this formative assessment, students are shown a Fictitious Student\\'s block-based code, where the Fictitious Student\\'s code is designed to model what happens when it rains by calculating the runoff and absorption levels based on the rainfall amount and absorption limit of the material the ground is made of. The students are then tasked with the following:\\n\\nIdentify and describe 5 errors in the Fictitious Student’s code.\\n\\nHere is the Fictitious Student\\'s code:\\n\\nwhen [Green Flag] clicked: \\t# (Line 1)\\n\\tset [Rainfall (inch)] to 1 \\t# (Line 2)\\n\\tif [Rainfall (inch)] == [Absorption Limit (inch)]:\\t# (Line 3)\\n\\t\\tset [Absorption (inch)] to [Rainfall (inch)]\\t# (Line 4)\\n\\t\\tset [Runoff (inch)] to 0\\t# (Line 5)\\n\\tset [Absorption Limit (inch) of the Selected Material]\\t# (Line 6)\\n\\tif [Rainfall (inch)] < [Absorption (inch)]:\\t# (Line 7)\\n\\t\\tset [Absorption (inch)] to [Absorption Limit (inch)]\\t# (Line 8)\\n\\t\\tset [Runoff (inch)] to 0\\t# (Line 9)\\n\\t\\tif [Rainfall (inch)] > [Absorption Limit (inch)]:\\t# (Line 10)\\n\\t\\t\\tset [Absorption Limit (inch)] to [Absorption (inch)]\\t# (Line 11)\\n\\t\\t\\tset [Runoff (inch)] to [Rainfall (inch)] - [Absorption Limit (inch)]\\t# (Line 12)\\n\\nHere are the 5 Errors in the Fictitious Student\\'s code:\\n\\n1. \"set [Absorption Limit (inch) of the Selected Material]\" in Line 6 is in the wrong place and needs to occur before the first conditional (i.e., before Line 3). This is because this piece of code defines the [Absorption Limit (inch)] constant, which must happen before [Absorption Limit (inch)] is first referenced. \\n2. For the \"less than\" condition on Line 7, [Rainfall (inch)] should be compared to [Absorption Limit (inch)] and not [Absorption (inch)].\\n3. In the \"less than\" condition, on Line 8 [Absorption (inch)] should be set to [Rainfall (inch)] instead of [Absorption Limit (inch)].\\n4. The \"greater than\" condition on Line 10 should not be nested within the \"less than\" condition on Line 7.\\n5. In the \"greater than\" condition, on Line 11 [Absorption (inch)] should be set to [Absorption Limit (inch)] and not the other way around (i.e., in the Fictitious Student\\'s code, the two need to be swapped).\\n\\nYou are to score student responses based on the following rubric to determine which of the 5 Errors each student correctly identifies:\\n\\nIncorrectly Set Absorption Limit [0 or 1]: 1 point if the student identifies that the absorption limit is initially set in the wrong part of the code or if the student mentions the absorption limit should be set before the first \"if\" statement. 0 points if the student does not identify this error.\\nRainfall Compared to Absorption [0 or 1]: 1 point if the student identifies that the code incorrectly compares rainfall to absorption in the \"less than\" condition on Line 7 or if the student mentions that rainfall should be compared to absorption limit instead of absorption. 0 points if the student does not identify this error.\\nAbsorption Set to Absorption Limit [0 or 1]: 1 point if the student identifies that, in the \"less than\" condition on Line 8, absorption is incorrectly set to absorption limit or that absorption should actually be set to rainfall. 0 points if the student does not identify this error.\\nGreater Than Nested in Less Than [0 or 1]: 1 point if the student identifies that the \"greater than\" conditional statement is incorrectly nested inside the \"less than\" conditional statement or that the \"greater than\" conditional should be connected to the \"less than\" conditional but not inside it. 0 points if the student does not identify this error. \\nAbsorption Limit Swapped With Absorption [0 or 1]: 1 point if the student identifies that, in the \"greater than\" condition on Line 11, absorption limit is incorrectly set to absorption or that absorption and absorption limit should actually be swapped (i.e., absorption should be set to absorption limit). 0 points if the student does not identify this error.\\n\\nBased on all of this information, score the following student\\'s formative assessment response pursuant to the rubric. Please provide your scores as a JSON document that conforms to the following schema:\\n\\n{\\n  \"incorrectly_set_absorption_limit\" : int,\\n  \"rainfall_compared_to_absorption\" : int,\\n  \"absorption_set_to_absorption_limit\" : int,\\n  \"greater_than_nested_in_less_than\" : int,\\n  \"absorption_limit_swapped_with_absorption\" : int\\n}\\n!@#@!\\nuser!~*~! The set absorption limit block is in the wrong place and should be before the first if condition.\\n\\nIn the less than condition, rainfall is incorrectly compared to absorption (it should be compared to absorption limit).\\n\\nIn the less than condition, absorption is set to absorption limit and should instead be set to rainfall.\\n\\nThe greater than condition is incorrectly nested inside the less than condition. It should be below the less than condition and not inside it.\\n\\nIn the greater than condition, absorption limit is incorrectly set to absorption. It should be the other way around: absorption should be set to absorption limit. \\n!@#@!\\nassistant!~*~!{\\n  \"incorrectly_set_absorption_limit\" : 1,\\n  \"rainfall_compared_to_absorption\" : 1,\\n  \"absorption_set_to_absorption_limit\" : 1,\\n  \"greater_than_nested_in_less_than\" : 1,\\n  \"absorption_limit_swapped_with_absorption\" : 1\\n}\\n!@#@!\\nuser!~*~!there are no numbers for the variables\\nit wont tell me if there\\'s a flood\\nthere is no runoff set for greater than  \\nnot sure\\ndont know\\n!@#@!\\nassistant!~*~!{\\n  \"incorrectly_set_absorption_limit\" : 0,\\n  \"rainfall_compared_to_absorption\" : 0,\\n  \"absorption_set_to_absorption_limit\" : 0,\\n  \"greater_than_nested_in_less_than\" : 0,\\n  \"absorption_limit_swapped_with_absorption\" : 0\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(PROMPT_PATH, 'r', encoding='utf-8') as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(messages):\n",
    "  start_time = time.time()\n",
    "  response = openai.ChatCompletion.create(\n",
    "      model=MODEL,\n",
    "      messages=messages,\n",
    "      temperature=0,\n",
    "      response_format={\"type\": \"json_object\"},\n",
    "      seed=SEED)\n",
    "  total_time = time.time()-start_time\n",
    "  total_tokens = response[\"usage\"][\"total_tokens\"]\n",
    "  generation = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "  return generation, total_time, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\n    \"working\": true,\\n    \"explanation\": \"The API is functioning correctly as there are no reported issues or errors in the current system status.\"\\n}', 17.12690830230713, 65)\n"
     ]
    }
   ],
   "source": [
    "response = get_openai_response([{\"role\":\"system\",\"content\":\"Confirm that the api is working. Respond using the following JSON schema: {'working':bool, 'explanation':str}\"}])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED FA3 STUDENT 0.\n",
      "FINISHED FA3 STUDENT 1.\n",
      "FINISHED FA3 STUDENT 2.\n",
      "FINISHED FA3 STUDENT 3.\n",
      "FINISHED FA3 STUDENT 4.\n",
      "FINISHED FA3 STUDENT 5.\n",
      "FINISHED FA3 STUDENT 6.\n",
      "FINISHED FA3 STUDENT 7.\n",
      "FINISHED FA3 STUDENT 8.\n",
      "FINISHED FA3 STUDENT 9.\n",
      "FINISHED FA3 STUDENT 10.\n",
      "FINISHED FA3 STUDENT 11.\n",
      "FINISHED FA3 STUDENT 12.\n",
      "FINISHED FA3 STUDENT 13.\n",
      "FINISHED FA3 STUDENT 14.\n",
      "FINISHED FA3 STUDENT 15.\n",
      "FINISHED FA3 STUDENT 16.\n",
      "FINISHED FA3 STUDENT 17.\n",
      "FINISHED FA3 STUDENT 18.\n",
      "FINISHED FA3 STUDENT 19.\n",
      "FINISHED FA3 STUDENT 20.\n",
      "FINISHED FA3 STUDENT 21.\n",
      "FINISHED FA3 STUDENT 22.\n",
      "FINISHED FA3 STUDENT 23.\n",
      "FINISHED FA3 STUDENT 24.\n",
      "FINISHED FA3 STUDENT 25.\n",
      "FINISHED FA3 STUDENT 26.\n",
      "FINISHED FA3 STUDENT 27.\n",
      "FINISHED FA3 STUDENT 28.\n",
      "FINISHED FA3 STUDENT 29.\n",
      "FINISHED FA3 STUDENT 30.\n",
      "FINISHED FA3 STUDENT 31.\n",
      "FINISHED FA3 STUDENT 32.\n",
      "FINISHED FA3 STUDENT 33.\n",
      "FINISHED FA3 STUDENT 34.\n",
      "FINISHED FA3 STUDENT 35.\n",
      "FINISHED FA3 STUDENT 36.\n",
      "FINISHED FA3 STUDENT 37.\n",
      "FINISHED FA3 STUDENT 38.\n",
      "FINISHED FA3 STUDENT 39.\n",
      "FINISHED FA3 STUDENT 40.\n",
      "FINISHED FA3 STUDENT 41.\n",
      "FINISHED FA3 STUDENT 42.\n",
      "FINISHED FA3 STUDENT 43.\n",
      "FINISHED FA3 STUDENT 44.\n",
      "FINISHED FA3 STUDENT 45.\n",
      "FINISHED FA3 STUDENT 46.\n",
      "FINISHED FA3 STUDENT 47.\n",
      "FINISHED FA3 STUDENT 48.\n",
      "FINISHED FA3 STUDENT 49.\n"
     ]
    }
   ],
   "source": [
    "results = [FA_COLS+[\"total_score\",\"total_time_s\",\"total_tokens\"]]\n",
    "\n",
    "prompt_messages = prompt.split(LINE_DELIM)\n",
    "assert len(prompt_messages) == N_PROMPT_MESSAGES\n",
    "\n",
    "system_role, system_content = prompt_messages[0].split(ROLE_CONTENT_DELIM)\n",
    "user_role1, user_content1 = prompt_messages[1].split(ROLE_CONTENT_DELIM)\n",
    "assistant_role1, assistant_content1 = prompt_messages[2].split(ROLE_CONTENT_DELIM)\n",
    "user_role2, user_content2 = prompt_messages[3].split(ROLE_CONTENT_DELIM)\n",
    "assistant_role2, assistant_content2 = prompt_messages[4].split(ROLE_CONTENT_DELIM)\n",
    "\n",
    "for idx,row in df.iterrows():\n",
    "  messages = [\n",
    "      {\"role\":system_role,\"content\":system_content},\n",
    "      {\"role\":user_role1,\"content\":user_content1},\n",
    "      {\"role\":assistant_role1,\"content\":assistant_content1},\n",
    "      {\"role\":user_role2,\"content\":user_content2},\n",
    "      {\"role\":assistant_role2,\"content\":assistant_content2},\n",
    "      {\"role\":\"user\",\"content\":row['response']}\n",
    "  ]\n",
    "\n",
    "  generation, total_time, total_tokens = get_openai_response(messages)\n",
    "  generation_data = json.loads(generation)\n",
    "\n",
    "  for k in FA_COLS:\n",
    "    FA_COLS_DICT[k] = int(generation_data[k])\n",
    "\n",
    "  total_score = sum(FA_COLS_DICT.values())\n",
    "\n",
    "  results.append([FA_COLS_DICT[col] for col in FA_COLS]+[total_score,total_time,total_tokens])\n",
    "\n",
    "  print(f\"FINISHED FA{FA} STUDENT {idx}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results[1:],columns=results[0])\n",
    "df = pd.concat([df,df_results],axis=1)\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf=RESULTS_PATH,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro‑F1: 0.7400\n",
      "QWK:      0.9445\n"
     ]
    }
   ],
   "source": [
    "y_true = df[\"score\"].astype(int)\n",
    "y_pred = df[\"total_score\"].astype(int)\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "\n",
    "print(f\"Micro‑F1: {f1:.4f}\")\n",
    "print(f\"QWK:      {qwk:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro‑F1: 74.00 ± 12.00\n",
      "QWK:      94.45 ± 4.01\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "f1_samples   = []\n",
    "kappa_samples = []\n",
    "\n",
    "n = len(df)\n",
    "\n",
    "for _ in range(N_BOOTSTRAP):\n",
    "    idx = rng.integers(0, n, n)\n",
    "    y_t = y_true.iloc[idx].values\n",
    "    y_p = y_pred.iloc[idx].values\n",
    "\n",
    "    f1_samples.append(\n",
    "        f1_score(y_t, y_p, average=\"micro\")\n",
    "    )\n",
    "    kappa_samples.append(\n",
    "        cohen_kappa_score(y_t, y_p, weights=\"quadratic\")\n",
    "    )\n",
    "\n",
    "ci_f1   = np.percentile(f1_samples, [2.5, 97.5])\n",
    "ci_kappa = np.percentile(kappa_samples, [2.5, 97.5])\n",
    "\n",
    "moe_f1   = (ci_f1[1]   - ci_f1[0])   / 2\n",
    "moe_kappa = (ci_kappa[1] - ci_kappa[0]) / 2\n",
    "\n",
    "print(f\"Micro‑F1: {f1*100:.2f} ± {moe_f1*100:.2f}\")\n",
    "print(f\"QWK:      {qwk*100:.2f} ± {moe_kappa*100:.2f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
